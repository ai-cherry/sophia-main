import pytest
import asyncio
from pathlib import Path
from unittest.mock import patch, AsyncMock

# Add project root to path to allow imports
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from backend.agents.iac_manager_agent import IaCManagerAgent
from backend.agents.core.base_agent import AgentConfig, Task
from backend.integrations.portkey_client import PortkeyClient

# Mock the filesystem tools to avoid actual file I/O during tests
@pytest.fixture
def mock_filesystem():
    with patch('backend.agents.tools.filesystem_tools.filesystem_tools') as mock_fs:
        mock_fs.read_file.return_value = "## Existing File Content"
        mock_fs.write_to_file.return_value = "Success"
        yield mock_fs

# Mock the agent router to isolate agent calls
@pytest.fixture
def mock_agent_router():
    with patch('backend.agents.iac_manager_agent.agent_router') as mock_router:
        # Mock the brain's code generation
        mock_brain_result = AsyncMock()
        mock_brain_result.status = "success"
        mock_brain_result.output = "\n# New code generated by AI\n"
        
        # Mock the pulumi agent's deployment call
        mock_pulumi_result = AsyncMock()
        mock_pulumi_result.status = "success"
        mock_pulumi_result.output = "Deployment triggered successfully."
        
        async def side_effect(agent_name, task):
            if agent_name == "brain_agent":
                return mock_brain_result
            if agent_name == "pulumi_agent":
                return mock_pulumi_result
            return None
            
        mock_router.route_command_to_agent.side_effect = side_effect
        yield mock_router

# Mock the Portkey client's LLM call
@pytest.fixture
def mock_portkey_client():
    with patch.object(PortkeyClient, 'llm_call', new_callable=AsyncMock) as mock_llm_call:
        # This is the JSON plan the LLM is expected to generate
        mock_plan = """
        [
            {
                "step": 1,
                "tool": "filesystem.read",
                "parameters": {"file_path": "infrastructure/lambda_labs.py"},
                "output_variable": "file_content"
            },
            {
                "step": 2,
                "tool": "brain.generate_code",
                "parameters": {"prompt": "Based on this content: $file_content, add a new GPU instance..."},
                "output_variable": "new_code"
            },
            {
                "step": 3,
                "tool": "filesystem.write",
                "parameters": {"file_path": "infrastructure/lambda_labs.py", "content": "$new_code", "append": true}
            },
            {
                "step": 4,
                "tool": "pulumi.deploy",
                "parameters": {"stack": "dev"}
            }
        ]
        """
        mock_llm_call.return_value = {
            "choices": [{"message": {"content": mock_plan}}]
        }
        yield mock_llm_call

@pytest.mark.asyncio
async def test_iac_manager_agent_full_execution(mock_filesystem, mock_agent_router, mock_portkey_client):
    """
    Tests the full end-to-end execution flow of the IaCManagerAgent.
    """
    # 1. Setup
    agent_config = AgentConfig(name="iac_manager_agent", version="1.0")
    # In a real test, the portkey_client would be a real or more sophisticated mock
    agent = IaCManagerAgent(config=agent_config, portkey_client=PortkeyClient()) 
    
    task = Task(
        command="create a new gpu instance on lambda labs for ai training",
        parameters={}
    )
    
    # 2. Execute
    result = await agent.execute_task(task)
    
    # 3. Assert
    assert result.status == "success"
    
    # Assert that the plan was generated
    mock_portkey_client.assert_awaited_once()
    
    # Assert that the filesystem was read
    mock_filesystem.read_file.assert_called_once_with(file_path="infrastructure/lambda_labs.py")
    
    # Assert that the brain was called to generate code
    # The first call to route_command_to_agent should be to the brain
    brain_call = mock_agent_router.route_command_to_agent.call_args_list[0]
    assert brain_call.args[0] == "brain_agent"
    assert "add a new GPU instance" in brain_call.args[1].parameters['prompt']
    
    # Assert that the file was written to
    mock_filesystem.write_to_file.assert_called_once()
    write_args = mock_filesystem.write_to_file.call_args[1]
    assert write_args['file_path'] == "infrastructure/lambda_labs.py"
    assert "# New code generated by AI" in write_args['content']
    assert write_args['append'] is True

    # Assert that pulumi deploy was called
    pulumi_call = mock_agent_router.route_command_to_agent.call_args_list[1]
    assert pulumi_call.args[0] == "pulumi_agent"
    assert pulumi_call.args[1].command == "deploy stack dev"
    
    # Assert the execution log looks correct
    assert "Step 1 (filesystem.read): SUCCESS" in result.output['execution_log']
    assert "Step 4 (pulumi.deploy): SUCCESS" in result.output['execution_log'] 